{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e7fadb5-3eae-4da8-ae0f-d1e4fb51e8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting holidays\n",
      "  Downloading holidays-0.74-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: python-dateutil in /opt/jaspy/lib/python3.11/site-packages (from holidays) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/jaspy/lib/python3.11/site-packages (from python-dateutil->holidays) (1.16.0)\n",
      "Downloading holidays-0.74-py3-none-any.whl (990 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m991.0/991.0 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: holidays\n",
      "Successfully installed holidays-0.74\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Purpose of this script is to create a benchmark neural net model against which to evaluate our XGBoost models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import signal\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "from functions import *\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "%pip install holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "629b0d54-9947-4504-a65b-aefc05765bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to set up and train a simple 3-layer feedforward neural network for each state\n",
    "\n",
    "def train_model(X, y, df, **kwargs):\n",
    "\t# Initialise the scaler\n",
    "\tscaler_X = MinMaxScaler()\n",
    "\tscaler_y = MinMaxScaler()\n",
    "\n",
    "\t# Apply the scaler to the features and target\n",
    "\tX_scaled = scaler_X.fit_transform(X)\n",
    "\ty_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "\tif 'year' in kwargs:\n",
    "\t\tyear_mask = df['date'].dt.year == kwargs['year']\n",
    "\t\tX_train, X_test, y_train, y_test = train_test_split(X_scaled[~year_mask], y_scaled[~year_mask], test_size=0.2, random_state=123)\n",
    "\t\tX_val, y_val = X_scaled[year_mask], y_scaled[year_mask]\n",
    "\t\n",
    "\telse:\n",
    "\t\tX_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=123)\n",
    "\t\tX_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=123)\n",
    "\n",
    "\t# Define neural network architecture\n",
    "\tmodel = Sequential([\n",
    "\t\tDense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "\t\tDense(64, activation='relu'),\n",
    "\t\tDense(32, activation='relu'),\n",
    "\t\tDense(1)\n",
    "\t])\n",
    "\n",
    "\t# Compile the model\n",
    "\tmodel.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "\t# Train the model\n",
    "\tmodel.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), batch_size=64) #verbose=0\n",
    "\n",
    "\t# Make predictions (scale back if necessary)\n",
    "\tpreds = model.predict(X_test).flatten()\n",
    "\n",
    "\treturn model, X_test, y_test, preds, scaler_X, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dd5242e-48a1-4d2d-95d9-c2365969eee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, preds, scaler_X, scaler_y):\n",
    "\n",
    "\tprint(y_test.shape, preds.shape)\n",
    "\n",
    "\trmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "\tsn = rmse / np.std(y_test)\n",
    "\tr_value = np.corrcoef(y_test.squeeze(), preds)[0, 1]\n",
    "\t\n",
    "\tprint(y_test.squeeze())\n",
    "\tprint(preds)\n",
    "\t\n",
    "\tprint(\"RMSE:\", rmse)\n",
    "\tprint(\"SN:\", sn)\n",
    "\tprint(\"R Value:\", r_value)\n",
    "\tprint('-' * 50)\n",
    "\n",
    "\treturn r_value, model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c05c2bc-57f7-4d3c-b2ea-d3d82458fc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andhra_Pradesh\n",
      "Epoch 1/100\n",
      "45/45 [==============================] - 1s 4ms/step - loss: 0.0510 - val_loss: 0.0098\n",
      "Epoch 2/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0090 - val_loss: 0.0080\n",
      "Epoch 3/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0079 - val_loss: 0.0075\n",
      "Epoch 4/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0073 - val_loss: 0.0073\n",
      "Epoch 5/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 6/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.0073\n",
      "Epoch 7/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0065 - val_loss: 0.0065\n",
      "Epoch 8/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0063 - val_loss: 0.0064\n",
      "Epoch 9/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0061 - val_loss: 0.0063\n",
      "Epoch 10/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0065 - val_loss: 0.0068\n",
      "Epoch 11/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 0.0061\n",
      "Epoch 12/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0058 - val_loss: 0.0062\n",
      "Epoch 13/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0058 - val_loss: 0.0059\n",
      "Epoch 14/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0058 - val_loss: 0.0060\n",
      "Epoch 15/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0056 - val_loss: 0.0060\n",
      "Epoch 16/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0056 - val_loss: 0.0060\n",
      "Epoch 17/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0057 - val_loss: 0.0073\n",
      "Epoch 18/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0056 - val_loss: 0.0060\n",
      "Epoch 19/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0055 - val_loss: 0.0058\n",
      "Epoch 20/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 0.0056\n",
      "Epoch 21/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0055 - val_loss: 0.0061\n",
      "Epoch 22/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0053 - val_loss: 0.0061\n",
      "Epoch 23/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0051 - val_loss: 0.0059\n",
      "Epoch 24/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0052 - val_loss: 0.0061\n",
      "Epoch 25/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0051 - val_loss: 0.0060\n",
      "Epoch 26/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0050 - val_loss: 0.0054\n",
      "Epoch 27/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0051 - val_loss: 0.0056\n",
      "Epoch 28/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0051 - val_loss: 0.0059\n",
      "Epoch 29/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0051 - val_loss: 0.0054\n",
      "Epoch 30/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0050 - val_loss: 0.0058\n",
      "Epoch 31/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 32/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0048 - val_loss: 0.0055\n",
      "Epoch 33/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0049 - val_loss: 0.0058\n",
      "Epoch 34/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0047 - val_loss: 0.0055\n",
      "Epoch 35/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0049 - val_loss: 0.0054\n",
      "Epoch 36/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0047 - val_loss: 0.0055\n",
      "Epoch 37/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0052 - val_loss: 0.0066\n",
      "Epoch 38/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0052 - val_loss: 0.0059\n",
      "Epoch 39/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0047 - val_loss: 0.0052\n",
      "Epoch 40/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0047 - val_loss: 0.0054\n",
      "Epoch 41/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0047 - val_loss: 0.0055\n",
      "Epoch 42/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0053\n",
      "Epoch 43/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0046 - val_loss: 0.0052\n",
      "Epoch 44/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0048 - val_loss: 0.0058\n",
      "Epoch 45/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0047 - val_loss: 0.0055\n",
      "Epoch 46/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0045 - val_loss: 0.0052\n",
      "Epoch 47/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0045 - val_loss: 0.0055\n",
      "Epoch 48/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0046 - val_loss: 0.0055\n",
      "Epoch 49/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0044 - val_loss: 0.0052\n",
      "Epoch 50/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0044 - val_loss: 0.0065\n",
      "Epoch 51/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0044 - val_loss: 0.0053\n",
      "Epoch 52/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0044 - val_loss: 0.0052\n",
      "Epoch 53/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.0057\n",
      "Epoch 54/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0047 - val_loss: 0.0057\n",
      "Epoch 55/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0046 - val_loss: 0.0053\n",
      "Epoch 56/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0050\n",
      "Epoch 57/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0053\n",
      "Epoch 58/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0041 - val_loss: 0.0052\n",
      "Epoch 59/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0042 - val_loss: 0.0054\n",
      "Epoch 60/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0043 - val_loss: 0.0051\n",
      "Epoch 61/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0042 - val_loss: 0.0053\n",
      "Epoch 62/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0041 - val_loss: 0.0060\n",
      "Epoch 63/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0041 - val_loss: 0.0053\n",
      "Epoch 64/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0041 - val_loss: 0.0051\n",
      "Epoch 65/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0051\n",
      "Epoch 66/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0043 - val_loss: 0.0059\n",
      "Epoch 67/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0040 - val_loss: 0.0052\n",
      "Epoch 68/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 69/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0043 - val_loss: 0.0052\n",
      "Epoch 70/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0040 - val_loss: 0.0051\n",
      "Epoch 71/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0045 - val_loss: 0.0055\n",
      "Epoch 72/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0040 - val_loss: 0.0053\n",
      "Epoch 73/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0053\n",
      "Epoch 74/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 75/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0042 - val_loss: 0.0055\n",
      "Epoch 76/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0051\n",
      "Epoch 77/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 78/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0058\n",
      "Epoch 79/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0051\n",
      "Epoch 80/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0039 - val_loss: 0.0052\n",
      "Epoch 81/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0039 - val_loss: 0.0054\n",
      "Epoch 82/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0052\n",
      "Epoch 83/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.0053\n",
      "Epoch 84/100\n",
      "45/45 [==============================] - 0s 1ms/step - loss: 0.0039 - val_loss: 0.0051\n",
      "Epoch 85/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0051\n",
      "Epoch 86/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0052\n",
      "Epoch 87/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0053\n",
      "Epoch 88/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0051\n",
      "Epoch 89/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0056\n",
      "Epoch 90/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0051\n",
      "Epoch 91/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0056\n",
      "Epoch 92/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0058\n",
      "Epoch 93/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0053\n",
      "Epoch 94/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0053\n",
      "Epoch 95/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0049\n",
      "Epoch 96/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0055\n",
      "Epoch 97/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0059\n",
      "Epoch 98/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0050\n",
      "Epoch 99/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0052\n",
      "Epoch 100/100\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.0056\n",
      "12/12 [==============================] - 0s 700us/step\n",
      "(359, 1) (359,)\n",
      "[0.50188792 0.52696996 0.61290577 0.64745655 0.72256245 0.83114406\n",
      " 0.88410178 0.66334289 0.70488643 0.54945734 0.69619221 0.66221205\n",
      " 0.69412495 0.50468969 0.51165966 0.67518541 0.20359952 0.66540269\n",
      " 0.60617286 0.61958255 0.80554509 0.68760298 0.59787071 0.47206334\n",
      " 0.54868831 0.57001744 0.70513777 0.6490219  0.55264039 0.6498445\n",
      " 0.6798614  0.60722772 0.7303436  0.77089554 0.74383158 0.69755363\n",
      " 0.61099134 0.61075449 0.65673263 0.60977821 0.54676746 0.51691366\n",
      " 0.62406241 0.55663613 0.64022113 0.57668891 0.64770749 0.58924496\n",
      " 0.50185844 0.62100608 0.68548576 0.49991304 0.58337536 0.90228963\n",
      " 0.59148992 0.49530952 0.61717044 0.61874853 0.74663137 0.48205213\n",
      " 0.63838855 0.52376512 0.54293909 0.17298233 0.50311267 0.57205113\n",
      " 0.64053419 0.4546451  0.72101519 0.30607638 0.69847802 0.40525736\n",
      " 0.60564086 0.67236998 0.56121769 0.63525124 0.53285998 0.79648439\n",
      " 0.57234094 0.6827381  0.6994356  0.63340178 0.67856334 0.57460281\n",
      " 0.78374564 0.74059279 0.70990077 0.53149133 0.62477068 0.60164286\n",
      " 0.6121176  0.31621258 0.44711089 0.66745445 0.67415484 0.47110798\n",
      " 0.68154246 0.63017645 0.49330457 0.60181217 0.61323028 0.5840891\n",
      " 0.45702215 0.63189506 0.76414803 0.68841647 0.66695997 0.70249933\n",
      " 0.508222   0.61503519 0.60078401 0.70674969 0.50468733 0.78017059\n",
      " 0.66863223 0.78353963 0.74220106 0.55701745 0.33502047 0.54629519\n",
      " 0.46060537 0.57879031 0.60609588 0.43588504 0.71604629 0.65239581\n",
      " 0.5792547  0.82820187 0.5620189  0.52642549 0.71389163 0.5451354\n",
      " 0.54202717 0.64186669 0.56520161 0.62829212 0.64161623 0.65088009\n",
      " 0.63148033 0.44998499 0.50619765 0.70496543 0.72632295 0.59794697\n",
      " 0.55881246 0.68324179 0.67120129 0.70933175 0.59007797 0.48377423\n",
      " 0.47157917 0.57649652 0.49896423 0.80569547 0.51768891 0.68588938\n",
      " 0.6541051  0.62082864 0.67138535 0.71192826 0.69970149 0.82783382\n",
      " 0.60703904 0.57301658 0.56292855 0.57013701 0.30371266 0.75340444\n",
      " 0.70130488 0.72977701 0.60121977 0.69106727 0.48707556 0.83580598\n",
      " 0.79985961 0.3750063  0.71455767 0.64468065 0.48592444 0.55306432\n",
      " 0.59826043 0.69067803 0.6144543  0.83233059 0.75884544 0.61986486\n",
      " 0.69470428 0.66752082 0.577255   0.3915728  0.78748817 0.65495283\n",
      " 0.49120933 0.52296361 0.61114237 0.75634468 0.53486766 0.59009796\n",
      " 0.65650539 0.63935689 0.69626704 0.56855564 0.77450704 0.74990237\n",
      " 0.69127484 0.09422872 0.70302229 0.58869016 0.68689327 0.46005982\n",
      " 0.59547675 0.7485938  0.45905039 0.57441216 0.64525981 0.48916991\n",
      " 0.3585419  0.77407885 0.61705493 0.57873717 0.75099234 0.53128882\n",
      " 0.58025979 0.56824864 0.49061468 0.33688192 0.70176565 0.75650007\n",
      " 0.61397515 0.78936662 0.7413432  0.61823576 0.81945892 0.38391366\n",
      " 0.85490952 0.72219696 0.6190791  0.58404551 0.59914932 0.57994553\n",
      " 0.59019317 0.70888691 0.76373642 0.46245323 0.69352392 0.\n",
      " 0.65451432 0.76153134 0.49864801 0.54764295 0.60458657 0.73255889\n",
      " 0.68445639 0.64204112 0.63605484 0.41120663 0.74258969 0.60781226\n",
      " 0.44413305 0.7171542  0.91209489 0.65274902 0.71538107 0.41927668\n",
      " 0.61309325 0.46851808 0.86641108 0.68460239 0.63890775 0.63473235\n",
      " 0.54742339 0.67345361 0.71534527 0.69577777 0.43372799 0.80117345\n",
      " 0.50616082 0.61045567 0.34444779 0.81895637 0.63006362 0.43150508\n",
      " 0.54477959 0.55249787 0.66107033 0.54917805 0.54109847 0.71815529\n",
      " 0.76146614 0.75411334 0.68030855 0.67280267 0.29991625 0.60226161\n",
      " 0.65756136 0.87307903 0.74603197 0.46676342 0.54327203 0.75116858\n",
      " 0.78527645 0.42214963 0.54666322 0.6625363  0.65504943 0.62885712\n",
      " 0.60745316 0.61004434 0.70198596 0.60057204 0.63394938 0.58662957\n",
      " 0.5119926  0.64725039 0.79637786 0.54069792 0.59915255 0.30136431\n",
      " 0.52683037 0.61260086 0.72119853 0.59496988 0.6344742  0.68208785\n",
      " 0.67796084 0.46145102 0.79102323 0.55273157 0.64833791 0.74947068\n",
      " 0.8002967  0.35915072 0.56712416 0.61992354 0.64473764 0.68732382\n",
      " 0.57889963 0.77042608 0.5678096  0.80287109 0.65477444 0.41861323\n",
      " 0.53336018 0.69785304 0.81697994 0.61229666 0.73210153 0.8326341\n",
      " 0.68029415 0.72853141 0.52066139 0.47387985 0.55650181 0.61804581\n",
      " 0.46097614 0.45999963 0.38024672 0.63835347 0.61301468]\n",
      "[0.5607267  0.56572753 0.5556465  0.72707635 0.6398911  0.71462196\n",
      " 0.8352663  0.7216394  0.6698845  0.5722745  0.68346715 0.64361155\n",
      " 0.63576734 0.46587572 0.4640433  0.64247525 0.40505144 0.71351594\n",
      " 0.55132735 0.6514247  0.7814671  0.6659764  0.57157016 0.5249946\n",
      " 0.64268523 0.46662942 0.6244335  0.6686335  0.57160115 0.6541635\n",
      " 0.68394995 0.60971165 0.7296005  0.74103194 0.6897954  0.76213855\n",
      " 0.73080814 0.61373746 0.64794534 0.57404256 0.49107912 0.53547716\n",
      " 0.5940256  0.5332181  0.5984705  0.6655467  0.70362586 0.50038326\n",
      " 0.48951566 0.49193296 0.71659184 0.6128741  0.68642414 0.7486262\n",
      " 0.5911479  0.5451612  0.5467064  0.55710363 0.7344618  0.5476779\n",
      " 0.5877131  0.5980298  0.57830954 0.36658242 0.40965196 0.45488372\n",
      " 0.64495265 0.50367767 0.67570263 0.5580167  0.73209417 0.46958852\n",
      " 0.6411339  0.649299   0.52955323 0.5999172  0.5173084  0.77937\n",
      " 0.58682734 0.62722987 0.6769421  0.66510254 0.6964772  0.59727764\n",
      " 0.7051555  0.70738435 0.80743873 0.56812274 0.6389655  0.6281714\n",
      " 0.6185611  0.5069334  0.53346074 0.7300558  0.69242257 0.2641753\n",
      " 0.76910436 0.6126722  0.4958873  0.5471283  0.5723523  0.6395096\n",
      " 0.4437195  0.6428547  0.69580805 0.7386246  0.62821436 0.72379667\n",
      " 0.5414942  0.6340909  0.5917119  0.6972384  0.5100571  0.7476194\n",
      " 0.5778502  0.75726265 0.6765118  0.59295654 0.41415933 0.5567778\n",
      " 0.48277172 0.61053145 0.66659373 0.47323918 0.6759191  0.52344817\n",
      " 0.53789675 0.8139279  0.5413639  0.58018506 0.6868917  0.4930798\n",
      " 0.5696225  0.63333523 0.545229   0.59519565 0.71012795 0.6710241\n",
      " 0.6175692  0.45748636 0.41785547 0.6720953  0.7269043  0.672215\n",
      " 0.64024544 0.7042508  0.6426575  0.75605583 0.51720816 0.5003326\n",
      " 0.5504288  0.70951474 0.5165734  0.6734716  0.48281404 0.57848716\n",
      " 0.6203462  0.565588   0.76304924 0.6880262  0.7339955  0.79571384\n",
      " 0.562302   0.5672948  0.53664416 0.55573934 0.6027827  0.7266041\n",
      " 0.7273734  0.84360087 0.524567   0.6985078  0.4443728  0.67155105\n",
      " 0.7337886  0.54224277 0.6758013  0.64862376 0.43672696 0.62915504\n",
      " 0.6326934  0.7070439  0.60683763 0.80819994 0.668944   0.60386837\n",
      " 0.70733446 0.6494105  0.49299058 0.58983636 0.74256814 0.65603447\n",
      " 0.61291623 0.5232655  0.53119403 0.7376648  0.59245825 0.59718835\n",
      " 0.6215798  0.63909847 0.6820943  0.6126898  0.6893199  0.7283952\n",
      " 0.68291235 0.20644253 0.71487117 0.62767565 0.56653726 0.6045222\n",
      " 0.59413755 0.7099488  0.5350814  0.5808147  0.72469616 0.52126646\n",
      " 0.4731822  0.70900476 0.7888721  0.5749292  0.6979736  0.53455204\n",
      " 0.53606254 0.4906955  0.5163949  0.5822538  0.63310146 0.7532356\n",
      " 0.6277076  0.7752525  0.6843953  0.6589676  0.7342032  0.542701\n",
      " 0.7178457  0.70705295 0.68313205 0.57877964 0.67033255 0.6742985\n",
      " 0.5712979  0.7202035  0.7492432  0.5036299  0.67694646 0.16439281\n",
      " 0.7071924  0.7726877  0.47857252 0.55427176 0.69294715 0.6685294\n",
      " 0.7058077  0.65199953 0.5892546  0.57218397 0.7346589  0.6420189\n",
      " 0.48469004 0.7937448  0.74442816 0.54076326 0.8126887  0.5131209\n",
      " 0.5990981  0.5173781  0.6948504  0.6615247  0.72140026 0.6702559\n",
      " 0.58087736 0.65915155 0.728065   0.7170213  0.47169533 0.7316026\n",
      " 0.5324761  0.5981078  0.58422714 0.7324386  0.6109118  0.48965713\n",
      " 0.6402528  0.56551147 0.681409   0.69823915 0.58594966 0.612624\n",
      " 0.7422412  0.7101102  0.71005464 0.7098743  0.3679379  0.6481559\n",
      " 0.7376418  0.7161293  0.64428735 0.5434314  0.6759217  0.64657605\n",
      " 0.6735543  0.45224664 0.5597595  0.637851   0.69357204 0.6526412\n",
      " 0.60348606 0.6182207  0.6844903  0.5698051  0.6890866  0.6415572\n",
      " 0.49828717 0.6597686  0.79314804 0.5709191  0.6516379  0.4791448\n",
      " 0.54769313 0.70744866 0.6973119  0.69015837 0.5435115  0.710086\n",
      " 0.68638253 0.48246485 0.7149036  0.5698815  0.6192477  0.6822561\n",
      " 0.7640661  0.4923988  0.604558   0.61742854 0.7416427  0.7270448\n",
      " 0.5499129  0.7525923  0.62324    0.7161497  0.68940806 0.5385269\n",
      " 0.53129333 0.63324916 0.70543027 0.6789235  0.74370027 0.7973206\n",
      " 0.6966218  0.7426484  0.55006754 0.52621746 0.5602717  0.59310997\n",
      " 0.46799865 0.56763864 0.514791   0.6601272  0.53193086]\n",
      "RMSE: 0.07211714692093103\n",
      "SN: 0.5712471447092237\n",
      "R Value: 0.8235567855567923\n",
      "--------------------------------------------------\n",
      "            State   R Value\n",
      "0  Andhra_Pradesh  0.823557\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model for each state, dump the results in a dict, and save to csv for plotting later\n",
    "\n",
    "results_dict = {\"State\": [], \"R Value\": [],}\n",
    "\n",
    "for state, subdiv in states_subdiv_mapping.items():\n",
    "\tprint(state)\n",
    "\t\n",
    "\tdf, df_energy = load_data(state)\n",
    "\tdf = process_data(df, df_energy, subdiv)\n",
    "\n",
    "\tdf = df[feature_names_full + ['date', 'energy_met_MU']]\n",
    "\tdf = df.dropna()\n",
    "\n",
    "\tX = df[feature_names_full]\n",
    "\ty = df[['energy_met_MU']]\n",
    "\n",
    "\tmodel, X_test, y_test, preds, scaler_X, scaler_y = train_model(X, y, df)  # Year parameter is optional\n",
    "\tr_value, n_parameters = evaluate_model(model, X_test, y_test, preds, scaler_X, scaler_y)\n",
    "\n",
    "\tresults_dict[\"State\"].append(state)\n",
    "\tresults_dict[\"R Value\"].append(r_value)\n",
    "\n",
    "\tbreak #just run for Andhra Pradesh as an example\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results_dict)\n",
    "print(results_df)\n",
    "results_df.to_csv(\"outputs/model_results_nn.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85536f34-c396-424d-9b16-e4d92253c2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Jaspy",
   "language": "python",
   "name": "jaspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
